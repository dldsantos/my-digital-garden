tipo: #artigo
lido: 12/07/2021
fonte: https://www.theatlantic.com/technology/archive/2017/03/aristotle-computer/518697/

## How Aristotle Created the Computer

Como meus professores costumavam fazer no colégio técnico, a história da [[Ciência da Computação]] continua sendo contada através de objetos como a *máquina analítica* de **Charles Babbage**, que foi o primeiro computador mecânico inventado, ou o computador que traduziu as mensagens alemãs para os ingleses durante a Segunda Guerra Mundial, concebido por **Alan Turing**.

Mas a [[Ciência da Computação]] também se originou a partir de ideias, principalmente originadas pela lógica matemática, que surgiu no século XIX puxada por filósofos-matemáticos como **George Boole** e **Gottlob Frege**, inspirados pelo sonho de  **Leibniz** de criar uma "linguagem conceitual" no sistema de [[lógica]] desenvolvido pelo filósofo grego **Aristóteles**.

Aristóteles descreveu a lógica em sua obra “**Órganon**”, e fez isso de uma forma tão completa que durante muito tempo acreditou-se que tudo o que havia para se escrever sobre o assunto já tivesse sido documentado.

Segundo Aristóteles, "*os argumentos são válidos, ou não, baseado em sua estrutura lógica, e independente das palavras não lógicas*'". Isso originou a proposição de [[silogismos]], base da [[lógica aristotélica]].

A evolução da [[Ciência da Computação]] a partir da lógica matemática culminou na década de 1930, com a publicação de dois importantes trabalhos por **Claude Shannon** e **Alan Turing**, ambos figuras proeminentes na área — só que a importância dos filósofos e pensadores lógicos que vieram antes deles normalmente é ignorada.

O trabalho de Shannon chamou a atenção à época por ter como referência principal 

> In Euclid’s system, geometric ideas were represented as spatial diagrams. Geometry continued to be practiced this way until René Descartes, in the 1630s, showed that geometry could instead be represented as formulas.

O pensamento euclidiano foi fortemente influenciado pelos axiomas propostos por Aristóteles.

A geometria euclidiana só foi alterada após 1630, depois que René Descartes mostrou que a geometria podia ser representada não só por diagramas, mas também por fórmulas. Foi também graças a Descartes que a notação algébrica que usamos até hoje (x, y e z são variáveis e a, b e c são quantidades conhecidas) surgiu.

> Boole replaced the words “men” and “mortal” with variables, and the logical words “all” and “are” with arithmetical operators:
> 
> x = x * y
> 
> Which could be interpreted as “Everything in the set x is also in the set y.”

Boole quis fazer por Aristóteles o que Descartes fez por Euclides: dar à lógica uma notação algébrica precisa para liberta-lá das limitações da intuição humana.

> Shannon’s insight was that Boole’s system could be mapped directly onto electrical circuits.

​

> Shannon realized that the right theory would be “exactly analogous to the calculus of propositions used in the symbolic study of logic.”

​

> This correspondence allowed computer scientists to import decades of work in logic and mathematics by Boole and subsequent logicians. In the second half of his paper, Shannon showed how Boolean logic could be used to create a circuit for adding two binary digits.

​

> These circuits would become the basic building blocks of what are now known as arithmetical logic units, a key component in modern computers.

​

> “the philosophy of one century is the common sense of the next.”

A questão é que não nos damos conta de que as coisas e conceitos que atualmente consideramos tão “comuns” e “básicas” já foram um dia consideradas perspicazes, reveladoras e esclarecedoras.

> he was first to distinguish between the logical and the physical layer of computers.

Shannon foi o primeiro a realizar esta distinção entre as camadas física e lógica dos computadores, algo que para a ciência da computação moderna é básico. 

> Since Shannon’s paper, a vast amount of progress has been made on the physical layer of computers, including the invention of the transistor in 1947 by William Shockley and his colleagues at Bell Labs. Transistors are dramatically improved versions of Shannon’s electrical relays — the best known way to physically encode Boolean operations

​

> A 2016 iPhone has about 3.3 billion transistors, each one a “relay switch” like those pictured in Shannon’s diagrams.

Isso somente foi possível porque nos 70 anos seguintes à invenção dos transistores, a indústria de semicondutores encontrou maneiras de agrupar mais e mais transistores em espaços cada vez menores.

> While Shannon showed how to map logic onto the physical world, Turing showed how to design computers in the language of mathematical logic.

​

> Inspired by Boole’s attempt to improve Aristotle’s logic, Frege developed a much more advanced logical system. The logic taught in philosophy and computer-science classes today—first-order or predicate logic—is only a slight modification of Frege’s system

​

> As Bertrand Russell famously quipped: “Mathematics may be defined as the subject in which we never know what we are talking about, nor whether what we are saying is true.”

​

> The major innovation of Frege’s logic is that it much more accurately represented the logical structure of ordinary language.

​

> Frege was the first to use quantifiers (“for every,” “there exists”) and to separate objects from predicates. He was also the first to develop what today are fundamental concepts in computer science like recursive functions and variables with scope and binding.

​

> An unexpected consequence of Frege’s work was the discovery of weaknesses in the foundations of mathematics.

​

> This realization created a crisis in the foundation of mathematics. If the Elements — the bible of mathematics — contained logical mistakes, what other fields of mathematics did too? What about sciences like physics that were built on top of mathematics?

​

> The good news is that the same logical methods used to uncover these errors could also be used to correct them. Mathematicians started rebuilding the foundations of mathematics from the bottom up. In 1889, Giuseppe Peano developed axioms for arithmetic, and in 1899, David Hilbert did the same for geometry.

​

> The historian Thomas Kuhn once observed that “in science, novelty emerges only with difficulty.”

​

> after Shannon and Turing’s insights—showing the connections between electronics, logic and computing—it was now possible to export this new conceptual machinery over to computer design.

​

> Turing joined a secret unit at Bletchley Park, northwest of London, where he helped design computers that were instrumental in breaking German codes. His most enduring contribution to practical computer design was his specification of the ACE, or Automatic Computing Engine.

​

> During World War II, this theoretical work was put into practice, when government labs conscripted a number of elite logicians

​

> Von Neumann joined the atomic bomb project at Los Alamos, where he worked on computer design to support physics research. In 1945, he wrote the specification of the EDVAC—the first stored-program, logic-based computer—which is generally considered the definitive source guide for modern computer design.

​

> Von Neumann’s favored designs were similar to modern CISC (“complex”) processors, baking rich functionality into hardware. Turing’s design was more like modern RISC (“reduced”) processors, minimizing hardware complexity and pushing more work to software

​

> Von Neumann thought computer programming would be a tedious, clerical job. Turing, by contrast, said computer programming “should be very fascinating. There need be no real danger of it ever becoming a drudge, for any processes that are quite mechanical may be turned over to the machine itself.”

​

> Logic began as a way to understand the laws of thought. It then helped create machines that could reason according to the rules of deductive logic. Today, deductive and inductive logic are being combined to create machines that both reason and learn. What began, in Boole’s words, with an investigation “concerning the nature and constitution of the human mind,” could result in the creation of new minds—artificial minds—that might someday match or even exceed our own.

​

